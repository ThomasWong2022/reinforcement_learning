{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from empyrical import max_drawdown, alpha_beta, sharpe_ratio, annual_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arctic import CHUNK_STORE, Arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-19 11:25:12,367\tERROR worker.py:643 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "# Start up Ray. This must be done before we instantiate any RL agents.\n",
    "ray.init(num_cpus=10, ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(price_source='Alpaca_Equity_daily',tickers=['SPY','QQQ'],start='2008-01-02',end='2010-01-02'):\n",
    "    '''Returned price data to use in gym environment'''\n",
    "    ## Load data \n",
    "    ## Each dataframe will have columns date and a collection of fields \n",
    "    if price_source in ['Alpaca_Equity_daily', 'Alpaca_Equity_minute', 'Quandl_Futures_daily']:\n",
    "        price_df = []\n",
    "        a = Arctic('localhost')\n",
    "        lib = a[price_source]\n",
    "        for t in tickers:\n",
    "            df1 = lib.read(t).set_index('date').loc[start:end]\n",
    "            price_df.append(df1[['Open','Volume']])\n",
    "    if price_source in ['csvdata']:\n",
    "        price_df = []\n",
    "        for t in tickers:\n",
    "            df1 = pd.read_csv('csvdata/{}.csv'.format(t)).set_index('date').loc[start:end]\n",
    "            price_df.append(df1)\n",
    "    if price_source in ['Alphavnatage_Equity_daily', 'Alphavnatage_Equity_minute',]:\n",
    "        price_df = []\n",
    "        a = Arctic('localhost')\n",
    "        lib = a[price_source]\n",
    "        for t in tickers:\n",
    "            df1 = lib.read(t).set_index('date').loc[start:end]\n",
    "            price_df.append(df1[['Close','Volume','Open','High','Low']])\n",
    "    \n",
    "    ## Merge data \n",
    "    ## Reference dataframe is taken from the first ticker read where the column labels are assumed to be the same\n",
    "    if len(price_df) > 0:\n",
    "        ref_df = price_df[0]\n",
    "        ref_df_columns = price_df[0].columns\n",
    "        for i in range(1,len(price_df)):\n",
    "            ref_df = ref_df.merge(price_df[i], how='outer', on='date',)\n",
    "        merged_df = ref_df.sort_values(by='date').fillna(0)\n",
    "    \n",
    "    ## Prepare price tensor for observation space \n",
    "    price_tensor = np.zeros(shape=(merged_df.shape[0],len(ref_df_columns),len(price_df)))\n",
    "    for count in range(len(price_df)):\n",
    "        price_tensor[:,:,count] = merged_df.values[:,len(ref_df_columns)*count:len(ref_df_columns)*(count+1)]\n",
    "        \n",
    "    return {'dates':merged_df.index, 'fields':ref_df_columns, 'data':price_tensor }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Equitydaily(gym.Env):\n",
    "\n",
    "    def __init__(self,env_config):\n",
    "        \n",
    "        self.tickers = env_config['tickers']\n",
    "        self.lookback = env_config['lookback']\n",
    "        # Load price data\n",
    "        price_data = load_data(env_config['pricing_source'],env_config['tickers'],env_config['start'],env_config['end'])\n",
    "        self.dates = price_data['dates']\n",
    "        self.fields = price_data['fields']\n",
    "        self.pricedata = price_data['data']\n",
    "        # Set up historical actions and rewards \n",
    "        self.n_assets = len(self.tickers) + 1\n",
    "        self.n_metrics = 2 \n",
    "        self.n_assets_fields = len(self.fields)\n",
    "        self.n_features = self.n_assets_fields * len(self.tickers) + self.n_assets + self.n_metrics # reward function\n",
    "        \n",
    "        # Set up action and observation space\n",
    "        # The last asset is cash \n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.tickers)+1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(self.lookback,self.n_features), dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        ## Normalise action space \n",
    "        normalised_action = action / np.sum(np.abs(action))\n",
    "        \n",
    "        done = False\n",
    "        # Rebalance portfolio at open, use log return of open price in the following day \n",
    "        next_day_log_return = self.pricedata[self.index+1,0,:]\n",
    "        # transaction cost \n",
    "        transaction_cost = self.transaction_cost(normalised_action,self.position_series[-1])\n",
    "        \n",
    "        # Rebalancing \n",
    "        self.position_series = np.append(self.position_series, [normalised_action], axis=0)\n",
    "        today_portfolio_return = np.sum(normalised_action[:-1] * next_day_log_return) + np.sum(transaction_cost)\n",
    "        self.log_return_series = np.append(self.log_return_series, [today_portfolio_return], axis=0)\n",
    "        \n",
    "        \n",
    "        # Calculate reward \n",
    "        # Need to cast log_return in pd series to use the functions in empyrical \n",
    "        live_days = self.index - self.lookback\n",
    "        burnin = 250\n",
    "        recent_series = pd.Series(self.log_return_series)[-100:]\n",
    "        whole_series = pd.Series(self.log_return_series)\n",
    "        if live_days > burnin: \n",
    "            self.metric = annual_return(whole_series) + 0.5* max_drawdown(whole_series)\n",
    "        else:\n",
    "            self.metric = annual_return(whole_series) + 0.5* max_drawdown(whole_series) *live_days / burnin\n",
    "        reward = self.metric - self.metric_series[-1]\n",
    "        #reward = self.metric\n",
    "        self.metric_series = np.append(self.metric_series, [self.metric], axis=0)\n",
    "        \n",
    "        # Check if the end of backtest\n",
    "        if self.index >= self.pricedata.shape[0]-2:\n",
    "            done = True\n",
    "            \n",
    "        # Prepare observation for next day\n",
    "        self.index += 1\n",
    "        price_lookback = self.pricedata[self.index-self.lookback:self.index,:,:].reshape(self.lookback,-1)\n",
    "        ## Smooth price_fields and add rolling volatility window \n",
    "        \n",
    "        \n",
    "        metrics = np.vstack((self.log_return_series[self.index-self.lookback:self.index], \n",
    "                             self.metric_series[self.index-self.lookback:self.index])).transpose()\n",
    "        self.observation = np.concatenate( (price_lookback,  metrics,\n",
    "                                            self.position_series[self.index-self.lookback:self.index]), axis=1)\n",
    "            \n",
    "            \n",
    "        return self.observation, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.log_return_series = np.zeros(shape=self.lookback)\n",
    "        self.metric_series = np.zeros(shape=self.lookback)\n",
    "        self.position_series = np.zeros(shape=(self.lookback,self.n_assets))\n",
    "        \n",
    "        self.metric = 0                    \n",
    "        self.index = self.lookback\n",
    "        # Observation join the price, metric and position \n",
    "        price_lookback = self.pricedata[:self.index,:,:].reshape(self.lookback,-1)\n",
    "        metrics = np.vstack((self.log_return_series, self.metric_series)).transpose()\n",
    "        self.observation = np.concatenate((price_lookback, metrics, self.position_series), axis=1)\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    # 0.05% t-cost for institutional portfolios \n",
    "    def transaction_cost(self,new_action,old_action,):\n",
    "        turnover = np.abs(new_action - old_action) \n",
    "        fees = 0.9995\n",
    "        tcost = turnover * np.log(fees)\n",
    "        return tcost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03921914100646973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.92202639579773"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "startt = time.time()\n",
    "load_data(tickers=['SPY','QQQ','SHY','GLD','TLT','LQD'],end='2018-12-31')\n",
    "time.time() - startt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model']['dim'] = 50\n",
    "config['model']['conv_filters'] = [[16, [5, 1], 1], [32, [5, 1], 5], [16, [10, 1], 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 1\n",
    "config[\"num_envs_per_worker\"] = 1\n",
    "config[\"rollout_fragment_length\"] = 20\n",
    "config[\"train_batch_size\"] = 5000\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "config['num_sgd_iter'] = 20\n",
    "config['sgd_minibatch_size'] = 200\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 2  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "config['env_config'] = {'pricing_source':'Alpaca_Equity_daily', 'tickers':['SPY','QQQ','SHY','GLD','TLT','LQD'], 'lookback':50, 'start':'2008-01-02', 'end':'2018-12-31'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 1,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'rollout_fragment_length': 20,\n",
       " 'batch_mode': 'complete_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 5000,\n",
       " 'model': {'fcnet_hiddens': [100, 100],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': [[16, [5, 1], 1], [32, [5, 1], 5], [16, [10, 1], 1]],\n",
       "  'conv_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action_reward': False,\n",
       "  '_time_major': False,\n",
       "  'framestack': True,\n",
       "  'dim': 50,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {'pricing_source': 'Alpaca_Equity_daily',\n",
       "  'tickers': ['SPY', 'QQQ', 'SHY', 'GLD', 'TLT', 'LQD'],\n",
       "  'lookback': 50,\n",
       "  'start': '2008-01-02',\n",
       "  'end': '2018-12-31'},\n",
       " 'env': None,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'no_eager_on_workers': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " '_use_trajectory_view_api': False,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 2,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent'},\n",
       " 'logger_config': None,\n",
       " 'replay_sequence_length': 1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 200,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 20,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': False,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if agents can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-19 12:58:52,033\tINFO trainable.py:252 -- Trainable.setup took 79.222 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-10-19 12:58:52,034\tWARNING util.py:39 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config, Equitydaily)\n",
    "best_reward = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleagent/checkpoint_1/checkpoint-1\n",
      "-0.6304444350903895\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    if result['episode_reward_mean'] > best_reward + 0.01:\n",
    "        path = agent.save('sampleagent')\n",
    "        print(path)\n",
    "        best_reward = result['episode_reward_mean']\n",
    "        print(best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': -0.561724132078505,\n",
       " 'episode_reward_min': -0.652029927175597,\n",
       " 'episode_reward_mean': -0.6075568131618514,\n",
       " 'episode_len_mean': 2717.0,\n",
       " 'episodes_this_iter': 2,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [-0.6076142503881217,\n",
       "   -0.561724132078505,\n",
       "   -0.652029927175597,\n",
       "   -0.6088589430051818],\n",
       "  'episode_lengths': [2717, 2717, 2717, 2717]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 1.1325459108908027,\n",
       "  'mean_raw_obs_processing_ms': 0.10075595122260311,\n",
       "  'mean_inference_ms': 0.9959995274910309,\n",
       "  'mean_action_processing_ms': 0.10824959068204312},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 10868,\n",
       " 'timers': {'sample_time_ms': 13268.656,\n",
       "  'sample_throughput': 409.537,\n",
       "  'load_time_ms': 51.685,\n",
       "  'load_throughput': 105137.179,\n",
       "  'learn_time_ms': 1744.321,\n",
       "  'learn_throughput': 3115.253,\n",
       "  'update_time_ms': 2.217},\n",
       " 'info': {'learner': {'default_policy': {'cur_kl_coeff': 0.20000000298023224,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': -0.07840514,\n",
       "    'policy_loss': -0.081929274,\n",
       "    'vf_loss': 0.0007026832,\n",
       "    'vf_explained_var': 0.09167413,\n",
       "    'kl': 0.01410727,\n",
       "    'entropy': 9.851078,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 10868,\n",
       "  'num_steps_trained': 10868},\n",
       " 'done': False,\n",
       " 'episodes_total': 4,\n",
       " 'training_iteration': 2,\n",
       " 'experiment_id': '7f91e00f6f744bc389ae35dfec2af059',\n",
       " 'date': '2020-10-19_12-59-22',\n",
       " 'timestamp': 1603108762,\n",
       " 'time_this_iter_s': 14.856367349624634,\n",
       " 'time_total_s': 30.210775136947632,\n",
       " 'pid': 186005,\n",
       " 'hostname': 'gauss',\n",
       " 'node_ip': '155.198.192.44',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 20,\n",
       "  'batch_mode': 'complete_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 5000,\n",
       "  'model': {'fcnet_hiddens': [100, 100],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': [[16, [5, 1], 1], [32, [5, 1], 5], [16, [10, 1], 1]],\n",
       "   'conv_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   '_time_major': False,\n",
       "   'framestack': True,\n",
       "   'dim': 50,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {'pricing_source': 'Alpaca_Equity_daily',\n",
       "   'tickers': ['SPY', 'QQQ', 'SHY', 'GLD', 'TLT', 'LQD'],\n",
       "   'lookback': 50,\n",
       "   'start': '2008-01-02',\n",
       "   'end': '2018-12-31'},\n",
       "  'env': 'Equitydaily',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  '_use_trajectory_view_api': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 2,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent'},\n",
       "  'logger_config': None,\n",
       "  'replay_sequence_length': 1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 200,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 20,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False},\n",
       " 'time_since_restore': 30.210775136947632,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 2,\n",
       " 'perf': {'cpu_util_percent': 10.586363636363636, 'ram_util_percent': 8.5}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-19 11:42:27,943\tINFO trainable.py:481 -- Restored on 155.198.192.44 from checkpoint: sampleagent/checkpoint_1/checkpoint-1\n",
      "2020-10-19 11:42:27,944\tINFO trainable.py:489 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 14.936065912246704, '_episodes_total': 2}\n"
     ]
    }
   ],
   "source": [
    "agent.restore('sampleagent/checkpoint_1/checkpoint-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    result = agent.train()\n",
    "    if result['episode_reward_mean'] > best_reward + 0.01:\n",
    "        path = agent.save('sampleagent')\n",
    "        print(path)\n",
    "        best_reward = result['episode_reward_mean']\n",
    "        print(best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': -1.4606538207848538,\n",
       " 'episode_reward_min': -1.4645063547769122,\n",
       " 'episode_reward_mean': -1.4626622356814447,\n",
       " 'episode_len_mean': 2717.0,\n",
       " 'episodes_this_iter': 2,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [-1.4636325476804446,\n",
       "   -1.4626063880059572,\n",
       "   -1.4630606312488108,\n",
       "   -1.4633451856324962,\n",
       "   -1.46218844444843,\n",
       "   -1.4617151830460218,\n",
       "   -1.4626371702060625,\n",
       "   -1.4626367047668551,\n",
       "   -1.4607733785111567,\n",
       "   -1.4627215988366584,\n",
       "   -1.4629789065195686,\n",
       "   -1.463788392166825,\n",
       "   -1.4637532287247266,\n",
       "   -1.4606538207848538,\n",
       "   -1.464005891906464,\n",
       "   -1.4645063547769122,\n",
       "   -1.461118710916315,\n",
       "   -1.4635861598443032,\n",
       "   -1.463104434903402,\n",
       "   -1.4620578211559732,\n",
       "   -1.4616518108417296,\n",
       "   -1.461373642925195,\n",
       "   -1.4625759648611647,\n",
       "   -1.4634212836443437],\n",
       "  'episode_lengths': [2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717,\n",
       "   2717]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 1.2044995966297296,\n",
       "  'mean_raw_obs_processing_ms': 0.10019501165771051,\n",
       "  'mean_inference_ms': 0.9066705293536313,\n",
       "  'mean_action_processing_ms': 0.10543741230819807},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 32604,\n",
       " 'timers': {'sample_time_ms': 12203.839,\n",
       "  'sample_throughput': 445.27,\n",
       "  'load_time_ms': 28.437,\n",
       "  'load_throughput': 191089.111,\n",
       "  'learn_time_ms': 1420.662,\n",
       "  'learn_throughput': 3824.977,\n",
       "  'update_time_ms': 2.484},\n",
       " 'info': {'learner': {'default_policy': {'cur_kl_coeff': 0.20000000298023224,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': -0.07343549,\n",
       "    'policy_loss': -0.0765482,\n",
       "    'vf_loss': 7.1840994e-05,\n",
       "    'vf_explained_var': 0.99539953,\n",
       "    'kl': 0.015204302,\n",
       "    'entropy': 9.725759,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 32604,\n",
       "  'num_steps_trained': 32604},\n",
       " 'done': False,\n",
       " 'episodes_total': 12,\n",
       " 'training_iteration': 6,\n",
       " 'experiment_id': '9d809acbdabe4bf48dfdbf3a2e7faaa7',\n",
       " 'date': '2020-10-19_11-43-38',\n",
       " 'timestamp': 1603104218,\n",
       " 'time_this_iter_s': 14.376010417938232,\n",
       " 'time_total_s': 83.2472620010376,\n",
       " 'pid': 186005,\n",
       " 'hostname': 'gauss',\n",
       " 'node_ip': '155.198.192.44',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 20,\n",
       "  'batch_mode': 'complete_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 5000,\n",
       "  'model': {'fcnet_hiddens': [100, 100],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   '_time_major': False,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {'pricing_source': 'Alpaca_Equity_daily',\n",
       "   'tickers': ['SPY', 'QQQ', 'SHY', 'GLD', 'TLT', 'LQD'],\n",
       "   'lookback': 50,\n",
       "   'start': '2008-01-02',\n",
       "   'end': '2018-12-31'},\n",
       "  'env': 'Equitydaily',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  '_use_trajectory_view_api': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 2,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent'},\n",
       "  'logger_config': None,\n",
       "  'replay_sequence_length': 1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 200,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 20,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False},\n",
       " 'time_since_restore': 68.3111960887909,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 5,\n",
       " 'perf': {'cpu_util_percent': 14.444999999999999,\n",
       "  'ram_util_percent': 19.909999999999997}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thomas/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.sac import SACTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 1\n",
    "config[\"num_envs_per_worker\"] = 1\n",
    "\n",
    "config[\"rollout_fragment_length\"] = 10\n",
    "config[\"train_batch_size\"] = 50\n",
    "config[\"timesteps_per_iteration\"] = 10\n",
    "config[\"buffer_size\"] = 10000\n",
    "\n",
    "config['Q_model']['fcnet_hiddens'] = [50, 50]\n",
    "config['policy_model']['fcnet_hiddens'] = [50, 50]\n",
    "config['num_cpus_per_worker'] = 5  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "config['env_config'] = {'pricing_source':'Alpaca_Equity_daily', 'tickers':['SPY','QQQ','SHY','GLD','TLT','LQD'], 'lookback':50, 'start':'2008-01-02', 'end':'2018-12-31'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-18 11:25:27,401\tWARNING sac_tf_policy.py:36 -- When not using a state-preprocessor with SAC, `fcnet_hiddens` will be set to an empty list! Any hidden layer sizes are defined via `policy_model.fcnet_hiddens` and `Q_model.fcnet_hiddens`.\n",
      "2020-10-18 11:25:29,626\tINFO trainable.py:252 -- Trainable.setup took 79.252 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-10-18 11:25:29,627\tWARNING util.py:39 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Train agent \n",
    "agent = SACTrainer(config, Equitydaily)\n",
    "best_reward = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.4620830970919299\n",
      "-1.4620830970919299\n",
      "-1.4620830970919299\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.462393085861623\n",
      "-1.4626715012281806\n",
      "-1.4626715012281806\n",
      "-1.4626715012281806\n",
      "-1.4626715012281806\n",
      "-1.4626715012281806\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    result = agent.train()\n",
    "    if result['episode_reward_mean'] > best_reward + 0.01:\n",
    "        path = agent.save('sampleagent')\n",
    "        print(path)\n",
    "        best_reward = result['episode_reward_mean']\n",
    "    print(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': -1.4603430197940985,\n",
       " 'episode_reward_min': -1.4644678157497302,\n",
       " 'episode_reward_mean': -1.4620830970919299,\n",
       " 'episode_len_mean': 2717.0,\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [-1.4603430197940985,\n",
       "   -1.4623457962025583,\n",
       "   -1.4621110663130388,\n",
       "   -1.4611477874002234,\n",
       "   -1.4644678157497302],\n",
       "  'episode_lengths': [2717, 2717, 2717, 2717, 2717]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 1.429634864225194,\n",
       "  'mean_raw_obs_processing_ms': 0.15369715130817593,\n",
       "  'mean_inference_ms': 1.0387717205894833,\n",
       "  'mean_action_processing_ms': 0.12727329722517183},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 15750,\n",
       " 'timers': {'learn_time_ms': 4.064,\n",
       "  'learn_throughput': 12301.743,\n",
       "  'update_time_ms': 4.777},\n",
       " 'info': {'learner': {'default_policy': {'mean_td_error': 0.28931177,\n",
       "    'actor_loss': -26.91102,\n",
       "    'critic_loss': 0.0787894,\n",
       "    'alpha_loss': -5.05177,\n",
       "    'alpha_value': 0.65155095,\n",
       "    'target_entropy': -7,\n",
       "    'mean_q': 23.948505,\n",
       "    'max_q': 25.215397,\n",
       "    'min_q': 14.065144,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 15750,\n",
       "  'num_steps_trained': 71300,\n",
       "  'last_target_update_ts': 15750,\n",
       "  'num_target_updates': 1426},\n",
       " 'done': False,\n",
       " 'episodes_total': 5,\n",
       " 'training_iteration': 80,\n",
       " 'experiment_id': 'e46d03729fd24b44a418d748e840dbbe',\n",
       " 'date': '2020-10-18_13-20-57',\n",
       " 'timestamp': 1603023657,\n",
       " 'time_this_iter_s': 0.9987564086914062,\n",
       " 'time_total_s': 84.93428444862366,\n",
       " 'pid': 186005,\n",
       " 'hostname': 'gauss',\n",
       " 'node_ip': '155.198.192.44',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 10,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 50,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   '_time_major': False,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {'pricing_source': 'Alpaca_Equity_daily',\n",
       "   'tickers': ['SPY', 'QQQ', 'SHY', 'GLD', 'TLT', 'LQD'],\n",
       "   'lookback': 50,\n",
       "   'start': '2008-01-02',\n",
       "   'end': '2018-12-31'},\n",
       "  'env': 'Equitydaily',\n",
       "  'normalize_actions': True,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 0.0001,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  '_use_trajectory_view_api': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 1,\n",
       "  'timesteps_per_iteration': 10,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 5,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent'},\n",
       "  'logger_config': None,\n",
       "  'replay_sequence_length': 1,\n",
       "  'twin_q': True,\n",
       "  'use_state_preprocessor': False,\n",
       "  'Q_model': {'fcnet_activation': 'relu', 'fcnet_hiddens': [50, 50]},\n",
       "  'policy_model': {'fcnet_activation': 'relu', 'fcnet_hiddens': [50, 50]},\n",
       "  'tau': 0.005,\n",
       "  'initial_alpha': 1.0,\n",
       "  'target_entropy': 'auto',\n",
       "  'n_step': 1,\n",
       "  'buffer_size': 10000,\n",
       "  'prioritized_replay': False,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'training_intensity': None,\n",
       "  'optimization': {'actor_learning_rate': 0.0003,\n",
       "   'critic_learning_rate': 0.0003,\n",
       "   'entropy_learning_rate': 0.0003},\n",
       "  'grad_clip': None,\n",
       "  'learning_starts': 1500,\n",
       "  'target_network_update_freq': 0,\n",
       "  'worker_side_prioritization': False,\n",
       "  '_deterministic_loss': False,\n",
       "  '_use_beta_distribution': False},\n",
       " 'time_since_restore': 84.93428444862366,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 80,\n",
       " 'perf': {'cpu_util_percent': 10.2, 'ram_util_percent': 19.4}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOTrainer(config, Equitydaily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Equitydaily({'pricing_source':'Alpaca_Equity_daily', 'tickers':['SPY','QQQ','SHY','GLD','TLT','EEM'], 'lookback':50, 'start':'2011-01-02', 'end':'2020-12-31'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.restore('checkpoint_1087/checkpoint-1087')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "reward_list = []\n",
    "cum_reward = 0\n",
    "actions = list()\n",
    "\n",
    "while not done:\n",
    "    #action = agent.compute_action(state)\n",
    "    action = np.array([0,0,0,0,0,0,1])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    actions.append(action)\n",
    "    reward_list.append(reward)\n",
    "\n",
    "pd.Series(env.log_return_series).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reward_list).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run environment for RNN environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Equitydaily({'pricing_source':'Alpaca_Equity_daily', 'tickers':['SPY','QQQ'], 'lookback':50, 'start':'2018-01-02', 'end':'2020-12-31'})\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "cum_reward = 0 \n",
    "actions = list()\n",
    "\n",
    "rnn_state = agent.get_policy().get_initial_state()\n",
    "\n",
    "while not done:\n",
    "    action, rnn_state, _ = agent.compute_action(state,rnn_state)\n",
    "    #action = np.array([1,-1])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    actions.append(actions)\n",
    "\n",
    "pd.Series(env.log_return_series).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_drawdown(pd.Series(env.log_return_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_return(pd.Series(env.log_return_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
